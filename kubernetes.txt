k8s

high availability, scalability, disaster recovery 

1 master node, many worker nodes 

on master node (control plane) is running API server, controller manager, scheduler, etcd, 

virtual network 

main kuber components :

node - virtual machine 

pod - smalest unit in k8s, abstraction over container 

service and ingress - for addressing connect to pods 

Service 
each pod has its own IP addr, but pods are ephemetal, are destroyed frequently
Service has stable IP addr, has loadbalansing, loose coupling, within and outside cluster 

ClusterIP service 
Incoming traffic: browser -> Ingress -> Service -> some pod 
Service communication: selector 
pods are identified via selectors , key-value pairs, labels of pods 
in case if we have 3 pods, svc matches all 3 replica, 
registers as Endpoints, must match all the selectors 
when we create Service, k8s creates Endpoint object with the same name as Service,
keeps track of which Pos are the members/endpoints of the Service 
ServicePort is arbitrary, targetPort must match the container port 

Headless service
client wants to communicate with 1 spec Pod directly
Pods want to talk directly with specific Pod 
So, not randomly selected
Use case: Stateful apps, like databases
IP address of each Pod: DNS lookup, returns single IP addr 

NodePort Service 
external traffic has access to fixed port on each Worker node
Range: 30000 - 32767
NodePort services are not secure

LoadBalancer service 
expose a service to the internet. 

ConfigMap - external configuration of app (url f.e)

secret - for storing secret data 

Volume - data storage, for DB 

Deploy - blueprint for app pods 

we create deployments, not pods 

Statefulset - for statfull apps or DBs

statful apps are DBs 

Deployment - for stateless apps

stateless apps don't keep record of state, each request is completely new 

stateless apps deployded using Deployment 

stateful apps deployed using StatefulSet 

Deployment vs StatefulSet 

replicating stateful aoos is more difficult 

Deployments: identical and interchangable, created in random order with random hashes,
one Service that load balanced to any Pod 

StatefulSet: more diffucult, can't be created/deleted at same time, can't be randomly addressed, 
replica Pods are not identical - Pod Identity
Every pod has it's own identifier, has fixed ordered names 
Next pod is only created, if previous is up and running 
deletion in reverce order, starting from the last one 
Pods endpoints have individual service name: ${pod name}.${govering service domain}
2 characteristics of pod: predicrable pod name, fixed individual DNS name 
when pod restarts, Ip addr changes, name and endpoint stays same 
Replicating stateful apps is complex, k8s helps you, need to do a lot 

Scalling DB apps:
we have 1 master pod with read/write options, other worker pods with only read options from Deployment apps
Pods do not use the same physical storage.
For data consistency we continuosly synchronizing of the data 
the worker mist know about each change to be up-to-date
data will be lost when all Pods die.
Persistent Volume lifecycle isn't tied to other component's lifecycle 
PV can be reattached to recreated pod 

worker machine on kuber cluster processes: container runtime, kubelet services, kube proxy

kubelet interacts with container and node 

kube proxy - forwards the requests 

master processes: api server, scheduler, controler manager, etcd 

kuber config is declarative 

Connfiguration file has 3 parts: metatdata, specification, status 

etcd - holds the current status of any kuber components, key-value store  (something like clister brain) 

minikube - run master and worker processes on 1 node 

install minicube 
curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube_latest_amd64.deb
sudo dpkg -i minikube_latest_amd64.deb

start minikube
minikube start --driver docker 

status minikube 
minikube status 

Deployment - abstraction over pods, we can't create pod, we create deployments 

before starting project we need to create 4 config files 

1 - ConfigMap with mongodb endpoint, 

2 - Secret - with mongodb user and pwd 

3 - Deployment and service for mongo app with internal service 

4 - Deployment and service for our own webapp with external service 

type of secret: Opaque	arbitrary user-defined data

for adding data as secret, we need to encode it 

echo -n mongouser | base64   #encode username 

deployment configuration file:

kind - deployment 

template - config for pod , has it's own metatdataand spec section 

label - we can give any k8s component a label. labels are key/value pair that are attached to k8s resources,
Identifier which should be meaningful and relevant to users 

label selector - identify a set of resources 

we can have multiple YAML configurations in 1 file separated by "---"

Service configuration file 

kind - Service 

name - an arbitrary name 

selector - select pods to forward the request to, 

app name  should maps with tne app name from Deployment 

targetPort is a pod port, should be the same as containerPort 

Deployment and service for webapp 

by default Services are internal. for making services available externally, we should specify type

type NodePort opens port for externall availability 

before running the cluster, we need to apply credentials and configuration file 

 kubectl apply -f mongo-config.yaml

next, apply secret configuration 

 kubectl apply -f mongo-secret.yaml
 
next, run mongo DB 

kubectl apply -f mongo.yaml

next, run webapp 

 kubectl apply -f webapp.yaml
 
 get all components of the cluster 
 
 kubectl get all 
 
 describe all info about some service 
 
 kubectl describe service webapp-service
 
 check logs on the pod 
 
 kubectl logs mongo-deployment-65ffdd9df6-sjz6h

for getting access to the webapp from the browser, we should know ip addr of minikube 

 minikube ip

also we can check it with command 

kubectl get node -o wide (more infor about thr node, also ip) 

for stopping pods , we can scale down to 0 it 

 kubectl scale --replicas=0 deployment/mongo-deployment
 
 next, we need to delete deployments 
 
 kubectl delete deploy mongo-deployment
 kubectl delete deploy webapp-deployment

kubectl scale --replicas=0 deployment/webapp-deployment

main kubectl commands:

Creation of nginx deployment 

 kubectl create deployment nginx-depl --image=nginx

Replicaset is managing replicas of a pod 

Layers of abstraction:

Deployment manages a replicaset 

ReplicaSet manages a pod 

Pod is an abstraction of Container 

Edit deployment config from CLI 

kubectl edit deployment nginx-depl
 
Login into some pod 
 
kubectl exec -it nginx-depl-7c8b44594d-jdgkj -- /bin/bash

Each config file has 3 parts: 

1. Metadata 

2. Specification 

3. Status 

Get status of the deployment in yaml format 

kubectl get deployment nginx-deployment -o yaml 

for connectivity to mongo database from mongo express, we need to specify 
ME_CONFIG_MONGODB_ADMINUSERNAME, ME_CONFIG_MONGODB_ADMINPASSWORD, 
ME_CONFIG_MONGODB_SERVER variables in deployment config 

for specifuing ME_CONFIG_MONGODB_SERVER we need to have configmap file 

type "Loadbalancer" for External service for outside access 

nodePort must be between 30000 - 32767

Namespaces

Organise resources in namespaces 

Namespaces are something like virtual cluster inside cluster 

4 namespaces per default:

kube-system - system processes , master, kubectl, etc 

kube-public - publicely access data, a cosnfigmap 

kube-node-lease - hearbeats of nodes , determines the availability of a node 

default - resources you create are located here 

Components, which can't be within a Namespace: Volume, Node, 

tool for namespaces: kubens 

create namespace

kubectl create namespace mongodb-namespace

kubens mongodb-namespace - switch into other namespace

Ingress explanation 

Ingress is for addressing external connection by domain name, not by ip address 

in host specification should be valid domain name, host map domain name to Node's ip addr, 
which is the entrypoint 

instead of kind Service we have kind Ingress

we need to install Ingress Controller in minikube for work with Ingress 

after installation Ingress controller, we need to create ingress rule 

by default, kubernetes-dashboard service is not installed. we can install it with 
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.1/aio/deploy/recommended.yaml

for resolving ip address for ingress, we need to enable ingress addons 
minikube addons enable ingress

for checking ip address of the ingress we can run
 kubectl get ingress -n kubernetes-dashboard --watch

Multiple paths for same host with TLS cert 

Helm explanation 

Packafe Manager for k8s

1 feature: sharing helm charts 

2 feature: templating engine - define a common blueprint and dynamic values are replaced by placeholders 

3. feature: some apps across diff envs 

Helm chart structure:

top level some_chart folder -> name of chart 

Chart.yaml -> meta info about chart 

values. yaml -> values from the template files 

charts folder -> chart dependencies 

templated folder -> the actual template files 

4 feature: release management - keeping track of all chart executions (only for helm V2)

Creation of helm chart:
helm install buildachart

Installation of the chart:
helm install my-cherry-chart buildachart/ --values buildachart/values.yaml

Uninstall (delete) of the chart:
 helm uninstall my-cherry-chart

Kubernetes volumes 

Persistent Volume, Persistent Volume Claim, Storage Class 

Storage recuirements:

1. storage doesn't depend on the pod lifecycle 

2. storage must be available on all nodes 

3. storage needs to survive even if cluster crashes. 

Persistent volume - a cluster resource created via YAML file. 

depending on storage type, spec attributes differ 

Persistent volumes are not namespaced 

k8s admin and k8s user can can manage volumes 

Apps has to claim the persistent volume

claims must be in the same namespace as a pod 

admin provisions storage resource, user creates claim to PV 

Storage Class - provisions PVs dynamically 

